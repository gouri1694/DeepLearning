{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP\n",
        "Natural Language Programming is a subfield of linguistics (human language), computer science and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data\n",
        "\n",
        "NLP is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. It is a component of artificial intelligence (AI).<br>\n",
        "\n",
        "NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in a number of fields, including medical research, search engines and business intelligence.\n"
      ],
      "metadata": {
        "id": "YInLajB5_aCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World Applications\n",
        "\n",
        "\n",
        "\n",
        "*   Contextual Advertisements: Based on analysis, targeted ads are shared\n",
        "*   Email Clients: spam filtering, smart reply\n",
        "*   Social Media: removing adult content, opinion mining\n",
        "*   Search engines: summary /one liner answer for searched content\n",
        "*   Chatbots\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MI9XWwu__Z_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common NLP Tasks\n",
        "\n",
        "\n",
        "1.   Text/Document Classification\n",
        "2.   Sentiment Analysis\n",
        "3.   Information retrieval\n",
        "4.   Parts of Speech Tagging\n",
        "1.   Language Detection and Machine Translation\n",
        "2.   Conversational Agents\n",
        "1.   Knowledge Graph and QA Systems\n",
        "2.   Text Summarization\n",
        "1.   Topic Modelling\n",
        "2.   Text Generation\n",
        "1.   Spell checking and Grammar correction\n",
        "2.   Text Parsing\n",
        "1.   Speech-To-Text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PqphgYBP_Z8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approaches to NLP\n",
        "1.   Heuristic Methods\n",
        "1.   Machine Learning Based Methods\n",
        "2.   Deep Learning Based Methods"
      ],
      "metadata": {
        "id": "bMDlc-ZlRy1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges in NLP\n",
        "\n",
        "\n",
        "1.   Ambiguity\n",
        "1.   Contextual Words\n",
        "1.   Colloquialisms and slang\n",
        "1.   Synonymns\n",
        "1.   Irony,Sarcasm and tonal difference\n",
        "2.   Spelling errors\n",
        "2.   Creativity\n",
        "2.   Diversity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eUMarLtqVdoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP pipeline is a set of steps followed to build an end to end NLP software.\n",
        "NLP software consist of following steps:\n",
        "\n",
        "\n",
        "1.   Data Acquisition\n",
        "2.   Text Preparation\n",
        "\n",
        "> *   Text Clean up block\n",
        "> *   Basic preprocessing\n",
        "> *   Advance Preprocessing\n",
        "\n",
        "3.   Feature ENgineering\n",
        "2.   Modelling\n",
        "\n",
        "> *   Model Building\n",
        "> *   Evaluation \n",
        "\n",
        "5.   Deployment\n",
        "\n",
        "> *   Deployment\n",
        "> *   Monitoring\n",
        "> *   Model Update\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L051ejG5W8FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing involves preparing and \"cleaning\" text data for machines to be able to analyze it. preprocessing puts data in workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including:\n",
        "- Tokenization. This is when text is broken down into smaller units to work with.\n",
        "- Stop word removal. This is when common words are removed from text so unique words that offer the most information about the text remain.\n",
        "- Lemmatization and stemming. This is when words are reduced to their root forms to process.\n",
        "- Part-of-speech tagging. This is when words are marked based on the part-of speech they are -- such as nouns, verbs and adjectives.\n"
      ],
      "metadata": {
        "id": "lcwXDiBrZUcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the data has been preprocessed, an algorithm is developed to process it. There are many different natural language processing algorithms, but two main types are commonly used:\n",
        "\n",
        "- Rules-based system. This system uses carefully designed linguistic rules. This approach was used early on in the development of natural language processing, and is still used.\n",
        "- Machine learning-based system. Machine learning algorithms use statistical methods. They learn to perform tasks based on training data they are fed, and adjust their methods as more data is processed. Using a combination of machine learning, deep learning and neural networks, natural language processing algorithms hone their own rules through repeated processing and learning.\n"
      ],
      "metadata": {
        "id": "nWa9gvztZWKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries \n",
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "XQliJuHtZVYd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text lowercase\n",
        "\n",
        "We do lowercase the text to reduce the size of the vocabulary of our text data."
      ],
      "metadata": {
        "id": "ajk5_EBDZaAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_text(text): \n",
        "    return text.lower() \n",
        "  \n",
        "input_str = 'We bought 6 equity shares last Month, and 5 are in profit.'\n",
        "lowercase_text(input_str) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lnaZmWSeZks5",
        "outputId": "21485e43-fe09-4b0d-d810-48eb15bdf21b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we bought 6 equity shares last month, and 5 are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Removing numbers \n",
        "def remove_num(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result \n",
        "  \n",
        "input_s = 'We bought 6 equity shares, and 5 are in profit.'\n",
        "remove_num(input_s) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "91QyKpm1ZlwB",
        "outputId": "0a354383-4e4e-4b70-bb6d-361b7a689463"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought  equity shares, and  are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the library \n",
        "import inflect \n",
        "q = inflect.engine() \n",
        "  \n",
        "# convert number into text \n",
        "def convert_num(text): \n",
        "    # split strings into list of texts \n",
        "    temp_string = text.split() \n",
        "    # initialise empty list \n",
        "    new_str = [] \n",
        "  \n",
        "    for word in temp_string: \n",
        "        # if text is a digit, convert the digit \n",
        "        # to numbers and append into the new_str list \n",
        "        if word.isdigit(): \n",
        "            temp = q.number_to_words(word) \n",
        "            new_str.append(temp) \n",
        "  \n",
        "        # append the texts as it is \n",
        "        else: \n",
        "            new_str.append(word) \n",
        "  \n",
        "    # join the texts of new_str to form a string \n",
        "    temp_str = ' '.join(new_str) \n",
        "    return temp_str \n",
        "  \n",
        "input_str = 'We bought 6 equity shares, and 5 are in profit.'\n",
        "convert_num(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jCzkfZ1VZrmr",
        "outputId": "a4dc78fe-b779-4c89-d5cb-aac18ac31afb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought six equity shares, and five are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's remove punctuation \n",
        "def rem_punct(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "  \n",
        "input_str = 'We bought 6 equity shares :-) :-)$$$$***!, and 5 are in profit:-) :-):-) :-):-) :-)!!.'\n",
        "rem_punct(input_str) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "78gNlwXHZt0D",
        "outputId": "4ea8690e-142e-44c6-d0b8-003da860f1b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought 6 equity shares   and 5 are in profit   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove default stopwords:\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ],
      "metadata": {
        "id": "QEXZ-fQvZynt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing nltk library\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "  \n",
        "# remove stopwords function \n",
        "def rem_stopwords(text): \n",
        "    stop_words = set(stopwords.words(\"english\")) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text \n",
        "  \n",
        "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
        "rem_stopwords(ex_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMOefTDZwGf",
        "outputId": "9537605c-fea0-439d-ad69-078f79723c3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "For Example: Mangoes ---> Mango\n",
        "\n",
        "             Boys ---> Boy\n",
        "             \n",
        "             going ---> go\n",
        "             \n",
        "             \n",
        "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
      ],
      "metadata": {
        "id": "kAG5PPK3Z5Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing nltk's porter stemmer \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "stem1 = PorterStemmer() \n",
        "  \n",
        "# stem words in the list of tokenised words \n",
        "def s_words(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    stems = [stem1.stem(word) for word in word_tokens] \n",
        "    return stems \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "s_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyeHM8HjZ1Yi",
        "outputId": "5faf9538-7dbc-4025-f5ef-93c08fae9b49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "paragrap=\"\"\"On the 8th International Day of Yoga, PM Modi participated in a mass yoga demonstration at Mysuru Palace Ground. Speaking on the occasion, the Prime Minister said that the yogic energy, which has been nurtured for centuries by the spiritual centers of India like Mysuru, is today giving direction to global health.PM Modi took part in a programme at Sri Suttur Math, Mysuru. He said, as per scripture, there is nothing as noble as knowledge that is why our sages shaped our consciousness that is laced with knowledge and adorned with science, the one that grows by enlightenment and gets stronger by research.\"\"\"\n",
        "sentences=nltk.sent_tokenize(paragrap) # Converts paragraphs into sentences\n",
        "print(sentences)\n",
        "# words=nltk.word_tokenize(paragrap) # Converts paragraphs into words\n",
        "# print(words)\n",
        "#Stemming is the process of reducing a word to its word stem\n",
        "# that affixes to suffixes and prefixes or to the roots of words known as a lemma\n",
        "stemmer=PorterStemmer()\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)\n",
        "print(sentences)\n",
        "print(set(stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFcyye58aCV1",
        "outputId": "d1ca0c2f-71bb-411b-c869-0a0c8dce9c8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['On the 8th International Day of Yoga, PM Modi participated in a mass yoga demonstration at Mysuru Palace Ground.', 'Speaking on the occasion, the Prime Minister said that the yogic energy, which has been nurtured for centuries by the spiritual centers of India like Mysuru, is today giving direction to global health.PM Modi took part in a programme at Sri Suttur Math, Mysuru.', 'He said, as per scripture, there is nothing as noble as knowledge that is why our sages shaped our consciousness that is laced with knowledge and adorned with science, the one that grows by enlightenment and gets stronger by research.']\n",
            "['on 8th intern day yoga , pm modi particip mass yoga demonstr mysuru palac ground .', 'speak occas , prime minist said yogic energi , nurtur centuri spiritu center india like mysuru , today give direct global health.pm modi took part programm sri suttur math , mysuru .', 'he said , per scriptur , noth nobl knowledg sage shape conscious lace knowledg adorn scienc , one grow enlighten get stronger research .']\n",
            "{\"doesn't\", 'too', 'our', 'them', 'why', 'no', 'but', 'during', 'have', 'this', 'the', 'about', 'weren', 'whom', 'shouldn', 'doesn', 'you', 'having', \"hasn't\", 'a', 'through', 'myself', 'and', 'very', 'own', 'more', \"you've\", 'he', 'hasn', 'mightn', \"shan't\", 'don', 'ain', 'to', 'herself', 'they', 'himself', 's', 'o', 'where', 'most', 'once', 'each', 'should', 'me', 'all', 'theirs', 'any', 'few', 'an', 'when', 'being', 'm', 'itself', 'him', 'only', \"mustn't\", 'needn', 'shan', 'themselves', 'been', \"haven't\", 'what', 'i', 'am', 'who', 'y', 'are', 'now', \"couldn't\", 'ours', 'or', 'those', 'on', 'won', 'wouldn', 'after', 'hadn', 'were', 'at', \"should've\", 'that', 'its', 'is', 'it', 'can', \"weren't\", 'off', 't', 'hers', 'which', 'of', 'did', \"it's\", \"that'll\", \"wasn't\", 'some', \"you'd\", 'll', 'has', 'just', 'from', 'other', 'couldn', 'below', 'doing', \"won't\", 'had', 'against', 'by', 'their', 'do', 'haven', 'didn', 'with', 'there', 'until', 'because', 're', 've', 'as', 'into', 'nor', 'was', 'out', 'such', \"shouldn't\", 'for', 'be', 'above', 'so', 'if', \"aren't\", \"needn't\", 'under', \"didn't\", \"isn't\", 'over', 'both', 'aren', 'up', \"she's\", 'further', 'yourselves', 'not', 'here', 'same', \"hadn't\", \"you're\", 'd', 'isn', 'these', 'while', 'will', 'than', 'does', 'then', 'his', 'down', \"mightn't\", 'wasn', \"you'll\", 'mustn', \"don't\", 'before', 'in', 'she', 'her', 'ma', \"wouldn't\", 'how', 'we', 'ourselves', 'your', 'between', 'my', 'yourself', 'again', 'yours'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming problem:** <br>\n",
        "\n",
        "Produced intermediate representation of word may not have any meaning. <br>\n",
        "eg: intelligen,fina etc"
      ],
      "metadata": {
        "id": "stSHcplbZ624"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
      ],
      "metadata": {
        "id": "Mq7PUWjUaH2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet \n",
        "from nltk.tokenize import word_tokenize \n",
        "lemma = wordnet.WordNetLemmatizer()\n",
        "nltk.download('all')\n",
        "# lemmatize string \n",
        "def lemmatize_word(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    # provide context i.e. part-of-speech(pos)\n",
        "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
        "    return lemmas \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DsFfZysaM40",
        "outputId": "f73d24dc-6842-4588-b5e4-65ac0cce8bd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'be',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolution',\n",
              " 'in',\n",
              " 'the',\n",
              " 'World',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individual',\n",
              " 'would',\n",
              " 'generate',\n",
              " 'terabytes',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize_word(paragrap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NC0yv7WaNif",
        "outputId": "01b7ab56-2cf7-420d-dc0d-f5d6fb5beea5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['On',\n",
              " 'the',\n",
              " '8th',\n",
              " 'International',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Yoga',\n",
              " ',',\n",
              " 'PM',\n",
              " 'Modi',\n",
              " 'participate',\n",
              " 'in',\n",
              " 'a',\n",
              " 'mass',\n",
              " 'yoga',\n",
              " 'demonstration',\n",
              " 'at',\n",
              " 'Mysuru',\n",
              " 'Palace',\n",
              " 'Ground',\n",
              " '.',\n",
              " 'Speaking',\n",
              " 'on',\n",
              " 'the',\n",
              " 'occasion',\n",
              " ',',\n",
              " 'the',\n",
              " 'Prime',\n",
              " 'Minister',\n",
              " 'say',\n",
              " 'that',\n",
              " 'the',\n",
              " 'yogic',\n",
              " 'energy',\n",
              " ',',\n",
              " 'which',\n",
              " 'have',\n",
              " 'be',\n",
              " 'nurture',\n",
              " 'for',\n",
              " 'centuries',\n",
              " 'by',\n",
              " 'the',\n",
              " 'spiritual',\n",
              " 'center',\n",
              " 'of',\n",
              " 'India',\n",
              " 'like',\n",
              " 'Mysuru',\n",
              " ',',\n",
              " 'be',\n",
              " 'today',\n",
              " 'give',\n",
              " 'direction',\n",
              " 'to',\n",
              " 'global',\n",
              " 'health.PM',\n",
              " 'Modi',\n",
              " 'take',\n",
              " 'part',\n",
              " 'in',\n",
              " 'a',\n",
              " 'programme',\n",
              " 'at',\n",
              " 'Sri',\n",
              " 'Suttur',\n",
              " 'Math',\n",
              " ',',\n",
              " 'Mysuru',\n",
              " '.',\n",
              " 'He',\n",
              " 'say',\n",
              " ',',\n",
              " 'as',\n",
              " 'per',\n",
              " 'scripture',\n",
              " ',',\n",
              " 'there',\n",
              " 'be',\n",
              " 'nothing',\n",
              " 'as',\n",
              " 'noble',\n",
              " 'as',\n",
              " 'knowledge',\n",
              " 'that',\n",
              " 'be',\n",
              " 'why',\n",
              " 'our',\n",
              " 'sag',\n",
              " 'shape',\n",
              " 'our',\n",
              " 'consciousness',\n",
              " 'that',\n",
              " 'be',\n",
              " 'lace',\n",
              " 'with',\n",
              " 'knowledge',\n",
              " 'and',\n",
              " 'adorn',\n",
              " 'with',\n",
              " 'science',\n",
              " ',',\n",
              " 'the',\n",
              " 'one',\n",
              " 'that',\n",
              " 'grow',\n",
              " 'by',\n",
              " 'enlightenment',\n",
              " 'and',\n",
              " 'get',\n",
              " 'stronger',\n",
              " 'by',\n",
              " 'research',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parts of Speech (POS) Tagging\n",
        "\n",
        "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. "
      ],
      "metadata": {
        "id": "ffDTFtZ9aU-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9b52473f-587e-48d8-9b8f-9047ef767bb1",
        "outputId": "20419e01-ce79-4f81-8111-c9ef8a74aa0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# importing tokenize library\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  \n",
        "# convert text into word_tokens with their tags \n",
        "def pos_tagg(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    return pos_tag(word_tokens) \n",
        "  \n",
        "pos_tagg('Are you afraid of something?') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894e9dcb-a59b-4417-80ee-c3b95556d307"
      },
      "source": [
        "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8a1e4606-392d-4346-81a9-c977fee94fb1",
        "outputId": "4b83c76f-070a-442e-c1b1-728a4a187f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# downloading the tagset  \n",
        "nltk.download('tagsets') \n",
        "  \n",
        "# extract information about the tag \n",
        "nltk.help.upenn_tagset('PRP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a848ef-1480-402f-8b8f-38d142f7d6ec"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "89f9bef2-7c25-4902-9b6e-57697899e4cd",
        "outputId": "81307534-23d3-4199-bfda-487fe4fbbee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk import pos_tag \n",
        "  \n",
        "# here we define chunking function with text and regular \n",
        "# expressions representing grammar as parameter \n",
        "def chunking(text, grammar): \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # label words with pos \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # create chunk parser using grammar \n",
        "    chunkParser = nltk.RegexpParser(grammar) \n",
        "  \n",
        "    # test it on the list of word tokens with tagged pos \n",
        "    tree = chunkParser.parse(word_pos) \n",
        "      \n",
        "    for subtree in tree.subtrees(): \n",
        "        print(subtree) \n",
        "    #tree.draw() \n",
        "      \n",
        "sentence = 'the little red parrot is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence, grammar) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fc7eb40-ceac-44f6-85d7-693d8df7943e"
      },
      "source": [
        "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
        "\n",
        "Libraries like Spacy and TextBlob are best for chunking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e550347f-f5b6-4011-8456-3da76ce620ac"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "34230428-369e-45fc-82b3-1187349fe952",
        "outputId": "58b78b4c-ace2-46e6-ca5d-154037354160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  test/NN\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  played/VBD\n",
            "  in/IN\n",
            "  between/IN\n",
            "  (ORGANIZATION WI/NNP)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Importing tokenization and chunk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag, ne_chunk \n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "  \n",
        "def ner(text): \n",
        "    # tokenize the text \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # pos tagging of words \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # tree of word entities \n",
        "    print(ne_chunk(word_pos)) \n",
        "  \n",
        "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
        "ner(text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22b47c63-1294-41e3-8bac-91a063b0af3e"
      },
      "source": [
        "# Text normalization\n",
        "\n",
        "In the tect pre-processing highly overlooked step is text normalization. The text normalization means the process of transforming the text into the canonical(or standard) form. Like, \"ok\" and \"k\" can be transformed to \"okay\", its canonical form.And another example is mapping of near identical words such as \"preprocessing\", \"pre-processing\" and \"pre processing\" to just \"preprocessing\".\n",
        "\n",
        "Text normaliztion is too useful for noisy textssuch as social media comments, comment to blog posts, text messages, where abbreviations, misspellings, and the use out-of-vocabulary(oov) are prevalent.\n",
        "\n",
        "\n",
        "### Effects of normalization\n",
        "\n",
        "Text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non-standard ways. We have also found it useful for topic extraction where near synonyms and spelling differences are common (like 'topic modelling', 'topic modeling', 'topic-modeling', 'topic-modelling').\n",
        "\n",
        "Unlike stemming and lemmatization, there is not a standard way to normalize texts. It typically depends on the task. For e.g, the way you would normalize clinical texts would arguably be different from how you normalize text messages.\n",
        "\n",
        "Some of the common approaches to text normalization include dictionary mappings, statistical machine translation (SMT) and spelling-correction based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb519a36-1dc6-4fd3-9aed-dad3aa139be8"
      },
      "source": [
        "# Word Count\n",
        "\n",
        "I am assuming you have the understanding of tokenization,the first figure we can calculate is the word frequency.By *word frequency* we can find out how many times each tokens appear in the text. When talking about word frequency, we distinguished between *types* and *tokens*.Types are the distinct words in a corpus, whereas tokens are the words, including repeats. Let's see how this works in practice.\n",
        "\n",
        "Let's take an example for better understanding:\n",
        "\n",
        "“There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,” the Prime Minister tweeted.\n",
        "\n",
        "How many tokens and types are there in above sentences?\n",
        "\n",
        "Let's use Python for calculating these figures. First, tokenize the sentence by using the tokenizer which uses the non-alphabetic characters as a separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7970fba5-e610-479f-a2e9-010f0b34afe3"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
        "m = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f563eb24-827c-4b77-99ec-efd4f13262a4",
        "outputId": "c0d21551-09f2-4e20-c29f-06a9f5738c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "tokens = WhitespaceTokenizer().tokenize(m)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9bc4d5d9-7dec-4115-8c62-29d818a2ada8",
        "outputId": "80efb983-c90a-438c-e054-49e06c6db81f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'There\",\n",
              " 'is',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'panic.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'work',\n",
              " 'together,',\n",
              " 'take',\n",
              " 'small',\n",
              " 'yet',\n",
              " 'important',\n",
              " 'measures',\n",
              " 'to',\n",
              " 'ensure',\n",
              " \"self-protection,'\",\n",
              " 'the',\n",
              " 'Prime',\n",
              " 'Minister',\n",
              " 'tweeted.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "83e8f2e4-b211-43b7-ba36-937ea4a43598",
        "outputId": "d8d91fd4-5475-4ad0-9f67-7617d8273148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "my_vocab = set(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "296a3e47-7ec2-42f3-9b25-e21f48e6a35f",
        "outputId": "6de60168-be65-42b9-d6a2-e3107bc7aec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'There\",\n",
              " 'Minister',\n",
              " 'Prime',\n",
              " 'We',\n",
              " 'ensure',\n",
              " 'important',\n",
              " 'is',\n",
              " 'measures',\n",
              " 'need',\n",
              " 'no',\n",
              " 'panic.',\n",
              " \"self-protection,'\",\n",
              " 'small',\n",
              " 'take',\n",
              " 'the',\n",
              " 'to',\n",
              " 'together,',\n",
              " 'tweeted.',\n",
              " 'work',\n",
              " 'yet'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "my_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "80958cfe-9c8d-44fe-9666-da8940620e73",
        "outputId": "91b1b109-67bc-4a33-8c78-5c2dfcfe10d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "my_st = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
        "#We'll import different tokenizer:\n",
        "from nltk.tokenize.regexp import WordPunctTokenizer\n",
        "#Above tokenizer also split the words into tokens:\n",
        "m_t = WordPunctTokenizer().tokenize(my_st)\n",
        "\n",
        "print(len(m_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "98a597df-ee1c-4478-b35c-74c3b5b00d34",
        "outputId": "3db43348-8247-491d-ecf5-19970b8ed69d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'\",\n",
              " 'There',\n",
              " 'is',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'panic',\n",
              " '.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'work',\n",
              " 'together',\n",
              " ',',\n",
              " 'take',\n",
              " 'small',\n",
              " 'yet',\n",
              " 'important',\n",
              " 'measures',\n",
              " 'to',\n",
              " 'ensure',\n",
              " 'self',\n",
              " '-',\n",
              " 'protection',\n",
              " \",'\",\n",
              " 'the',\n",
              " 'Prime',\n",
              " 'Minister',\n",
              " 'tweeted',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "m_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "727c3845-1f52-4952-8e36-23aad80e21f2",
        "outputId": "71830955-eb5f-43de-88d6-5a5a24246cc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "my_vocab = set(m_t)\n",
        "print(len(my_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35b1dca-5af8-4748-b5d1-74c80a470bf4"
      },
      "source": [
        "# Frequency distribution\n",
        "\n",
        "What is Frequency distribution? This is basically counting words in your texts.To give a brief example of how it works,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a5bcfd4c-f7a8-43da-ad44-1dba6b6fc21a",
        "outputId": "fb5f727b-8a1f-418c-9c3b-b165d0260c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "<FreqDist with 23 samples and 28 outcomes>\n"
          ]
        }
      ],
      "source": [
        "#from nltk.book import *\n",
        "import nltk\n",
        "#nltk.download('gutenberg')\n",
        "print(\"\\n\\n\\n\")\n",
        "text1 = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
        "freqDist = nltk.FreqDist(word_tokenize(text1))\n",
        "print(freqDist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071a64c4-85cc-4080-ae74-212b3df9b8dd"
      },
      "source": [
        "The class **FreqDist** works like a dictionary where keys are the words in the text and the values are count associated with that word. For example, if you want to see how many words \"person\" are in the text, you can type as:"
      ]
    }
  ]
}