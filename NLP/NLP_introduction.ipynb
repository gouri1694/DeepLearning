{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP\n",
        "Natural Language Programming is a subfield of linguistics (human language), computer science and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data\n",
        "\n",
        "NLP is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. It is a component of artificial intelligence (AI).<br>\n",
        "\n",
        "NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in a number of fields, including medical research, search engines and business intelligence.\n"
      ],
      "metadata": {
        "id": "YInLajB5_aCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World Applications\n",
        "\n",
        "\n",
        "\n",
        "*   Contextual Advertisements: Based on analysis, targeted ads are shared\n",
        "*   Email Clients: spam filtering, smart reply\n",
        "*   Social Media: removing adult content, opinion mining\n",
        "*   Search engines: summary /one liner answer for searched content\n",
        "*   Chatbots\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MI9XWwu__Z_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common NLP Tasks\n",
        "\n",
        "\n",
        "1.   Text/Document Classification\n",
        "2.   Sentiment Analysis\n",
        "3.   Information retrieval\n",
        "4.   Parts of Speech Tagging\n",
        "1.   Language Detection and Machine Translation\n",
        "2.   Conversational Agents\n",
        "1.   Knowledge Graph and QA Systems\n",
        "2.   Text Summarization\n",
        "1.   Topic Modelling\n",
        "2.   Text Generation\n",
        "1.   Spell checking and Grammar correction\n",
        "2.   Text Parsing\n",
        "1.   Speech-To-Text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PqphgYBP_Z8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approaches to NLP\n",
        "1.   Heuristic Methods\n",
        "1.   Machine Learning Based Methods\n",
        "2.   Deep Learning Based Methods"
      ],
      "metadata": {
        "id": "bMDlc-ZlRy1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges in NLP\n",
        "\n",
        "\n",
        "1.   Ambiguity\n",
        "1.   Contextual Words\n",
        "1.   Colloquialisms and slang\n",
        "1.   Synonymns\n",
        "1.   Irony,Sarcasm and tonal difference\n",
        "2.   Spelling errors\n",
        "2.   Creativity\n",
        "2.   Diversity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eUMarLtqVdoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP pipeline is a set of steps followed to build an end to end NLP software.\n",
        "NLP software consist of following steps:\n",
        "\n",
        "\n",
        "1.   Data Acquisition\n",
        "2.   Text Preparation\n",
        "\n",
        "> *   Text Clean up block\n",
        "> *   Basic preprocessing\n",
        "> *   Advance Preprocessing\n",
        "\n",
        "3.   Feature ENgineering\n",
        "2.   Modelling\n",
        "\n",
        "> *   Model Building\n",
        "> *   Evaluation \n",
        "\n",
        "5.   Deployment\n",
        "\n",
        "> *   Deployment\n",
        "> *   Monitoring\n",
        "> *   Model Update\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L051ejG5W8FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing involves preparing and \"cleaning\" text data for machines to be able to analyze it. preprocessing puts data in workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including:\n",
        "- Tokenization. This is when text is broken down into smaller units to work with.\n",
        "- Stop word removal. This is when common words are removed from text so unique words that offer the most information about the text remain.\n",
        "- Lemmatization and stemming. This is when words are reduced to their root forms to process.\n",
        "- Part-of-speech tagging. This is when words are marked based on the part-of speech they are -- such as nouns, verbs and adjectives.\n"
      ],
      "metadata": {
        "id": "lcwXDiBrZUcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the data has been preprocessed, an algorithm is developed to process it. There are many different natural language processing algorithms, but two main types are commonly used:\n",
        "\n",
        "- Rules-based system. This system uses carefully designed linguistic rules. This approach was used early on in the development of natural language processing, and is still used.\n",
        "- Machine learning-based system. Machine learning algorithms use statistical methods. They learn to perform tasks based on training data they are fed, and adjust their methods as more data is processed. Using a combination of machine learning, deep learning and neural networks, natural language processing algorithms hone their own rules through repeated processing and learning.\n"
      ],
      "metadata": {
        "id": "nWa9gvztZWKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries \n",
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "XQliJuHtZVYd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text lowercase\n",
        "\n",
        "We do lowercase the text to reduce the size of the vocabulary of our text data."
      ],
      "metadata": {
        "id": "ajk5_EBDZaAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_text(text): \n",
        "    return text.lower() \n",
        "  \n",
        "input_str = 'We bought 6 equity shares last Month, and 5 are in profit.'\n",
        "lowercase_text(input_str) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lnaZmWSeZks5",
        "outputId": "75f7994f-d42e-4011-d1ff-1a0e6a39ceef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we bought 6 equity shares last month, and 5 are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Removing numbers \n",
        "def remove_num(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result \n",
        "  \n",
        "input_s = 'We bought 6 equity shares, and 5 are in profit.'\n",
        "remove_num(input_s) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "91QyKpm1ZlwB",
        "outputId": "fccbb2f2-3886-474c-aaa0-765dbc799853"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought  equity shares, and  are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the library \n",
        "import inflect \n",
        "q = inflect.engine() \n",
        "  \n",
        "# convert number into text \n",
        "def convert_num(text): \n",
        "    # split strings into list of texts \n",
        "    temp_string = text.split() \n",
        "    # initialise empty list \n",
        "    new_str = [] \n",
        "  \n",
        "    for word in temp_string: \n",
        "        # if text is a digit, convert the digit \n",
        "        # to numbers and append into the new_str list \n",
        "        if word.isdigit(): \n",
        "            temp = q.number_to_words(word) \n",
        "            new_str.append(temp) \n",
        "  \n",
        "        # append the texts as it is \n",
        "        else: \n",
        "            new_str.append(word) \n",
        "  \n",
        "    # join the texts of new_str to form a string \n",
        "    temp_str = ' '.join(new_str) \n",
        "    return temp_str \n",
        "  \n",
        "input_str = 'We bought 6 equity shares, and 5 are in profit.'\n",
        "convert_num(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jCzkfZ1VZrmr",
        "outputId": "d7fe7206-3755-4158-bf52-fc351d464559"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought six equity shares, and five are in profit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's remove punctuation \n",
        "def rem_punct(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "  \n",
        "input_str = 'We bought 6 equity shares :-) :-)$$$$***!, and 5 are in profit:-) :-):-) :-):-) :-)!!.'\n",
        "rem_punct(input_str) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "78gNlwXHZt0D",
        "outputId": "9fc395cf-4e48-4118-c520-a32fef384b0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We bought 6 equity shares   and 5 are in profit   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove default stopwords:\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ],
      "metadata": {
        "id": "QEXZ-fQvZynt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing nltk library\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "  \n",
        "# remove stopwords function \n",
        "def rem_stopwords(text): \n",
        "    stop_words = set(stopwords.words(\"english\")) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text \n",
        "  \n",
        "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
        "rem_stopwords(ex_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMOefTDZwGf",
        "outputId": "053c5801-b6df-4464-dad4-4fe8417117b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "For Example: Mangoes ---> Mango\n",
        "\n",
        "             Boys ---> Boy\n",
        "             \n",
        "             going ---> go\n",
        "             \n",
        "             \n",
        "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
      ],
      "metadata": {
        "id": "kAG5PPK3Z5Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing nltk's porter stemmer \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "stem1 = PorterStemmer() \n",
        "  \n",
        "# stem words in the list of tokenised words \n",
        "def s_words(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    stems = [stem1.stem(word) for word in word_tokens] \n",
        "    return stems \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "s_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyeHM8HjZ1Yi",
        "outputId": "8abf09b8-06ef-4c8a-eebb-78cbd3ee9277"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "paragrap=\"\"\"On the 8th International Day of Yoga, PM Modi participated in a mass yoga demonstration at Mysuru Palace Ground. Speaking on the occasion, the Prime Minister said that the yogic energy, which has been nurtured for centuries by the spiritual centers of India like Mysuru, is today giving direction to global health.PM Modi took part in a programme at Sri Suttur Math, Mysuru. He said, as per scripture, there is nothing as noble as knowledge that is why our sages shaped our consciousness that is laced with knowledge and adorned with science, the one that grows by enlightenment and gets stronger by research.\"\"\"\n",
        "sentences=nltk.sent_tokenize(paragrap) # Converts paragraphs into sentences\n",
        "print(sentences)\n",
        "# words=nltk.word_tokenize(paragrap) # Converts paragraphs into words\n",
        "# print(words)\n",
        "#Stemming is the process of reducing a word to its word stem\n",
        "# that affixes to suffixes and prefixes or to the roots of words known as a lemma\n",
        "stemmer=PorterStemmer()\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)\n",
        "print(sentences)\n",
        "print(set(stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFcyye58aCV1",
        "outputId": "2fe902b0-2ba7-429f-baef-f021c59afca5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['On the 8th International Day of Yoga, PM Modi participated in a mass yoga demonstration at Mysuru Palace Ground.', 'Speaking on the occasion, the Prime Minister said that the yogic energy, which has been nurtured for centuries by the spiritual centers of India like Mysuru, is today giving direction to global health.PM Modi took part in a programme at Sri Suttur Math, Mysuru.', 'He said, as per scripture, there is nothing as noble as knowledge that is why our sages shaped our consciousness that is laced with knowledge and adorned with science, the one that grows by enlightenment and gets stronger by research.']\n",
            "['on 8th intern day yoga , pm modi particip mass yoga demonstr mysuru palac ground .', 'speak occas , prime minist said yogic energi , nurtur centuri spiritu center india like mysuru , today give direct global health.pm modi took part programm sri suttur math , mysuru .', 'he said , per scriptur , noth nobl knowledg sage shape conscious lace knowledg adorn scienc , one grow enlighten get stronger research .']\n",
            "{'hasn', 'shan', 'do', 'yourselves', 'in', \"aren't\", \"shouldn't\", 'once', \"needn't\", 'myself', 'during', 't', 'you', 'few', 'before', 'this', 'has', 'most', 'there', 'why', 'were', 's', \"hadn't\", \"she's\", 'wasn', \"didn't\", 'he', 'if', 'than', 'weren', 'i', 'she', 'while', 'then', 'where', \"isn't\", 'its', 'at', 'y', 'from', 'doing', 'these', 'being', 'didn', 'any', 'herself', \"doesn't\", 'no', 'been', 'through', \"mustn't\", \"haven't\", 'under', 'ma', 'just', 'itself', \"it's\", 'are', 'his', 'wouldn', 'was', 'won', \"couldn't\", 'himself', 'can', 'did', 'needn', 'off', 'same', 'have', 'of', \"wasn't\", 'whom', 'until', 'about', 'by', 'what', 'for', \"mightn't\", 'when', 'll', 'having', 'the', 'each', 'down', 'and', 'their', 'my', 'yourself', 'doesn', 'is', 'your', 'shouldn', \"you'd\", 'ourselves', 'all', \"that'll\", \"don't\", 'should', \"should've\", 'couldn', 'out', 'aren', 'after', 'theirs', 'up', 'such', 'those', 'very', 'that', 'as', 'm', 'nor', 'him', 'ain', 've', 'over', 'both', 'yours', 'other', 'not', 'me', 'some', 'haven', 'our', 'does', 'how', 'mightn', \"won't\", 'be', 'between', 'a', 'on', 'here', 'further', 'but', 'now', 'who', 'isn', 'to', 'o', \"hasn't\", 'into', 're', 'with', 'too', \"you've\", 'because', 'hers', 'above', 'which', 'again', 'mustn', 'am', 'or', 'd', \"weren't\", 'more', 'we', 'them', 'had', 'only', 'below', 'own', 'will', 'against', \"you'll\", 'themselves', \"shan't\", 'her', 'an', \"you're\", 'they', 'so', \"wouldn't\", 'hadn', 'it', 'don', 'ours'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming problem:** <br>\n",
        "\n",
        "Produced intermediate representation of word may not have any meaning. <br>\n",
        "eg: intelligen,fina etc"
      ],
      "metadata": {
        "id": "stSHcplbZ624"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
      ],
      "metadata": {
        "id": "Mq7PUWjUaH2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet \n",
        "from nltk.tokenize import word_tokenize \n",
        "lemma = wordnet.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "# lemmatize string \n",
        "def lemmatize_word(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    # provide context i.e. part-of-speech(pos)\n",
        "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
        "    return lemmas \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "id": "4DsFfZysaM40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize_word(paragrap)"
      ],
      "metadata": {
        "id": "6NC0yv7WaNif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parts of Speech (POS) Tagging\n",
        "\n",
        "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. "
      ],
      "metadata": {
        "id": "ffDTFtZ9aU-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b52473f-587e-48d8-9b8f-9047ef767bb1",
        "outputId": "f4852164-3783-4c16-ebd6-a5befc5e153b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing tokenize library\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  \n",
        "# convert text into word_tokens with their tags \n",
        "def pos_tagg(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    return pos_tag(word_tokens) \n",
        "  \n",
        "pos_tagg('Are you afraid of something?') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894e9dcb-a59b-4417-80ee-c3b95556d307"
      },
      "source": [
        "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a1e4606-392d-4346-81a9-c977fee94fb1",
        "outputId": "138054d5-cb79-42da-e5d0-61379bf6a5f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# downloading the tagset  \n",
        "nltk.download('tagsets') \n",
        "  \n",
        "# extract information about the tag \n",
        "nltk.help.upenn_tagset('PRP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a848ef-1480-402f-8b8f-38d142f7d6ec"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89f9bef2-7c25-4902-9b6e-57697899e4cd",
        "outputId": "f5f8d3a6-6525-431e-9a90-1eda14f4dd17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk import pos_tag \n",
        "  \n",
        "# here we define chunking function with text and regular \n",
        "# expressions representing grammar as parameter \n",
        "def chunking(text, grammar): \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # label words with pos \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # create chunk parser using grammar \n",
        "    chunkParser = nltk.RegexpParser(grammar) \n",
        "  \n",
        "    # test it on the list of word tokens with tagged pos \n",
        "    tree = chunkParser.parse(word_pos) \n",
        "      \n",
        "    for subtree in tree.subtrees(): \n",
        "        print(subtree) \n",
        "    #tree.draw() \n",
        "      \n",
        "sentence = 'the little red parrot is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence, grammar) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fc7eb40-ceac-44f6-85d7-693d8df7943e"
      },
      "source": [
        "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
        "\n",
        "Image after running above code.\n",
        "![11.png](attachment:a134a770-562e-44d8-ac2b-a44d8bd9de31.png)\n",
        "\n",
        "Libraries like Spacy and TextBlob are best for chunking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e550347f-f5b6-4011-8456-3da76ce620ac"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34230428-369e-45fc-82b3-1187349fe952",
        "outputId": "d07159b8-878d-45b0-ac6a-419d154058e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  test/NN\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  played/VBD\n",
            "  in/IN\n",
            "  between/IN\n",
            "  (ORGANIZATION WI/NNP)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "#Importing tokenization and chunk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag, ne_chunk \n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "  \n",
        "def ner(text): \n",
        "    # tokenize the text \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # pos tagging of words \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # tree of word entities \n",
        "    print(ne_chunk(word_pos)) \n",
        "  \n",
        "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
        "ner(text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22b47c63-1294-41e3-8bac-91a063b0af3e"
      },
      "source": [
        "# Text normalization\n",
        "\n",
        "In the tect pre-processing highly overlooked step is text normalization. The text normalization means the process of transforming the text into the canonical(or standard) form. Like, \"ok\" and \"k\" can be transformed to \"okay\", its canonical form.And another example is mapping of near identical words such as \"preprocessing\", \"pre-processing\" and \"pre processing\" to just \"preprocessing\".\n",
        "\n",
        "Text normaliztion is too useful for noisy textssuch as social media comments, comment to blog posts, text messages, where abbreviations, misspellings, and the use out-of-vocabulary(oov) are prevalent.\n",
        "\n",
        "![15.png](attachment:08fa258a-1415-4a13-8cab-03085726b101.png)\n",
        "\n",
        "### Effects of normalization\n",
        "\n",
        "Text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non-standard ways. We have also found it useful for topic extraction where near synonyms and spelling differences are common (like 'topic modelling', 'topic modeling', 'topic-modeling', 'topic-modelling').\n",
        "\n",
        "Unlike stemming and lemmatization, there is not a standard way to normalize texts. It typically depends on the task. For e.g, the way you would normalize clinical texts would arguably be different from how you normalize text messages.\n",
        "\n",
        "Some of the common approaches to text normalization include dictionary mappings, statistical machine translation (SMT) and spelling-correction based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb519a36-1dc6-4fd3-9aed-dad3aa139be8"
      },
      "source": [
        "# Word Count\n",
        "\n",
        "I am assuming you have the understanding of tokenization,the first figure we can calculate is the word frequency.By *word frequency* we can find out how many times each tokens appear in the text. When talking about word frequency, we distinguished between *types* and *tokens*.Types are the distinct words in a corpus, whereas tokens are the words, including repeats. Let's see how this works in practice.\n",
        "\n",
        "Let's take an example for better understanding:\n",
        "\n",
        "“There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,” the Prime Minister tweeted.\n",
        "\n",
        "How many tokens and types are there in above sentences?\n",
        "\n",
        "Let's use Python for calculating these figures. First, tokenize the sentence by using the tokenizer which uses the non-alphabetic characters as a separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7970fba5-e610-479f-a2e9-010f0b34afe3"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
        "m = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f563eb24-827c-4b77-99ec-efd4f13262a4",
        "outputId": "d9d5e980-863e-440e-c7a4-0f802b707b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "tokens = WhitespaceTokenizer().tokenize(m)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bc4d5d9-7dec-4115-8c62-29d818a2ada8",
        "outputId": "0d3267f7-fd97-4b84-9653-5349f6917e93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"'There\",\n",
              " 'is',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'panic.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'work',\n",
              " 'together,',\n",
              " 'take',\n",
              " 'small',\n",
              " 'yet',\n",
              " 'important',\n",
              " 'measures',\n",
              " 'to',\n",
              " 'ensure',\n",
              " \"self-protection,'\",\n",
              " 'the',\n",
              " 'Prime',\n",
              " 'Minister',\n",
              " 'tweeted.']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83e8f2e4-b211-43b7-ba36-937ea4a43598",
        "outputId": "e8dbef3d-4a9b-4023-ceaf-f3f6602fb0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "my_vocab = set(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "296a3e47-7ec2-42f3-9b25-e21f48e6a35f",
        "outputId": "f50adbb9-25b3-465a-877b-b67080186c97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\"'There\",\n",
              " 'Minister',\n",
              " 'Prime',\n",
              " 'We',\n",
              " 'ensure',\n",
              " 'important',\n",
              " 'is',\n",
              " 'measures',\n",
              " 'need',\n",
              " 'no',\n",
              " 'panic.',\n",
              " \"self-protection,'\",\n",
              " 'small',\n",
              " 'take',\n",
              " 'the',\n",
              " 'to',\n",
              " 'together,',\n",
              " 'tweeted.',\n",
              " 'work',\n",
              " 'yet'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80958cfe-9c8d-44fe-9666-da8940620e73",
        "outputId": "8b0f8a13-43e8-4844-91de-1dc4a0ed50bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "my_st = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
        "#We'll import different tokenizer:\n",
        "from nltk.tokenize.regexp import WordPunctTokenizer\n",
        "#Above tokenizer also split the words into tokens:\n",
        "m_t = WordPunctTokenizer().tokenize(my_st)\n",
        "\n",
        "print(len(m_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98a597df-ee1c-4478-b35c-74c3b5b00d34",
        "outputId": "679ec81f-90bc-4f30-8a9e-7ce2f4dffce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"'\",\n",
              " 'There',\n",
              " 'is',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'panic',\n",
              " '.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'work',\n",
              " 'together',\n",
              " ',',\n",
              " 'take',\n",
              " 'small',\n",
              " 'yet',\n",
              " 'important',\n",
              " 'measures',\n",
              " 'to',\n",
              " 'ensure',\n",
              " 'self',\n",
              " '-',\n",
              " 'protection',\n",
              " \",'\",\n",
              " 'the',\n",
              " 'Prime',\n",
              " 'Minister',\n",
              " 'tweeted',\n",
              " '.']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727c3845-1f52-4952-8e36-23aad80e21f2",
        "outputId": "34c9af2f-bbb4-47ca-96d5-3fe99d78716a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "my_vocab = set(m_t)\n",
        "print(len(my_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35b1dca-5af8-4748-b5d1-74c80a470bf4"
      },
      "source": [
        "# Frequency distribution\n",
        "\n",
        "What is Frequency distribution? This is basically counting words in your texts.To give a brief example of how it works,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5bcfd4c-f7a8-43da-ad44-1dba6b6fc21a",
        "outputId": "7e205b02-a494-4242-a538-99849bd946cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "<FreqDist with 23 samples and 28 outcomes>\n"
          ]
        }
      ],
      "source": [
        "#from nltk.book import *\n",
        "import nltk\n",
        "#nltk.download('gutenberg')\n",
        "print(\"\\n\\n\\n\")\n",
        "text1 = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
        "freqDist = nltk.FreqDist(word_tokenize(text1))\n",
        "print(freqDist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071a64c4-85cc-4080-ae74-212b3df9b8dd"
      },
      "source": [
        "The class **FreqDist** works like a dictionary where keys are the words in the text and the values are count associated with that word. For example, if you want to see how many words \"person\" are in the text, you can type as:"
      ]
    }
  ]
}